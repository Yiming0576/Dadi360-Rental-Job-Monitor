import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
import time
import schedule
import os
import json # ç”¨äºæŒä¹…åŒ–å·²å‘é€çš„æˆ¿æº ID
import urllib3 # ç”¨äºæŠ‘åˆ¶SSLè­¦å‘Š
from loguru import logger
import sys
import re # ç”¨äºæ—¥æœŸåŒ¹é…
from collections import defaultdict
from datetime import datetime

# é…ç½®loguruæ—¥å¿—
logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨

# è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")

# ä¿¡æ¯æ—¥å¿— - è®°å½•æ‰€æœ‰INFOåŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
logger.add(
    os.path.join(LOGS_DIR, "rental_info.log"),
    rotation="10 MB",
    retention="30 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    encoding="utf-8",
    filter=lambda record: record["level"].name in ["INFO", "SUCCESS", "WARNING"]
)

# é”™è¯¯æ—¥å¿— - åªè®°å½•ERRORåŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
logger.add(
    os.path.join(LOGS_DIR, "rental_error.log"),
    rotation="5 MB",
    retention="60 days",
    level="ERROR",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    encoding="utf-8",
    filter=lambda record: record["level"].name in ["ERROR", "CRITICAL"]
)

# æ§åˆ¶å°è¾“å‡º - æ˜¾ç¤ºæ‰€æœ‰INFOåŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
logger.add(
    sys.stdout,
    level="INFO",
    format="{time:HH:mm:ss} | {level} | {message}",
    colorize=True
)

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs(LOGS_DIR, exist_ok=True)

# æŠ‘åˆ¶SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®ä¿¡æ¯ä» config.json åŠ è½½ ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), '../../config.json')
with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
    CONFIG = json.load(f)

# --- è¾…åŠ©å‡½æ•°ï¼šæŒä¹…åŒ–å·²å‘é€ID ---
def load_sent_ids(file_path):
    """ä»æ–‡ä»¶ä¸­åŠ è½½å·²å‘é€çš„æˆ¿æºIDé›†åˆã€‚"""
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"åŠ è½½å·²å‘é€IDæ–‡ä»¶å¤±è´¥ {file_path}: {e}. åˆå§‹åŒ–ä¸ºç©ºé›†åˆã€‚")
            return set()
    return set()

def save_sent_ids(file_path, ids_set):
    """å°†å·²å‘é€çš„æˆ¿æºIDé›†åˆä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(list(ids_set), f, ensure_ascii=False, indent=4)
    except IOError as e:
        logger.error(f"ä¿å­˜å·²å‘é€IDæ–‡ä»¶å¤±è´¥ {file_path}: {e}")

# åˆå§‹åŒ–å·²å‘é€IDé›†åˆ
# è¿™æ˜¯ä¸€ä¸ªå¤–éƒ¨çŠ¶æ€ï¼Œä½†åœ¨ä¸»å‡½æ•°ä¸­ä¼šé€šè¿‡å‚æ•°ä¼ é€’ï¼Œå°½é‡å‡å°‘ç›´æ¥ä¾èµ–
SENT_IDS_FILE = os.path.join(os.path.dirname(__file__), "sent_listing_ids.json")
_sent_listing_ids = load_sent_ids(SENT_IDS_FILE)


# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def fetch_html(url: str, headers: dict) -> str | None:
    """
    çº¯å‡½æ•°ï¼šæ ¹æ®URLå’Œè¯·æ±‚å¤´è·å–ç½‘é¡µçš„HTMLå†…å®¹ã€‚
    - è¾“å…¥ï¼šurl, headers
    - è¾“å‡ºï¼šHTMLå­—ç¬¦ä¸²æˆ–None
    - å‰¯ä½œç”¨ï¼šæ— 
    """
    try:
        logger.info(f"æ­£åœ¨è¯·æ±‚: {url}")
        # æ·»åŠ  verify=False æ¥è·³è¿‡SSLè¯ä¹¦éªŒè¯
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚ {url} å¤±è´¥: {e}")
        return None

def parse_html_for_listings(html_content: str, base_url: str, search_terms: list) -> list[dict]:
    """
    çº¯å‡½æ•°ï¼šè§£æHTMLå†…å®¹ï¼Œæå–ç¬¦åˆæ¡ä»¶çš„æˆ¿æºä¿¡æ¯ã€‚
    - è¾“å…¥ï¼šHTMLå­—ç¬¦ä¸², ç½‘ç«™åŸºç¡€URL, æœç´¢å…³é”®è¯åˆ—è¡¨
    - è¾“å‡ºï¼šç¬¦åˆæ¡ä»¶çš„æˆ¿æºåˆ—è¡¨ (æ¯ä¸ªå…ƒç´ æ˜¯ {'title': '...', 'link': '...', 'author': '...', 'date': '...'})
    - å‰¯ä½œç”¨ï¼šæ— 
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    listings = []

    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æˆ¿æºä¿¡æ¯çš„è¡¨æ ¼è¡Œ
    # æ¯ä¸ªæˆ¿æºéƒ½åœ¨ <tr class="bg_small_yellow"> ä¸­
    topic_rows = soup.find_all('tr', class_='bg_small_yellow')

    for row in topic_rows:
        # åœ¨æ¯è¡Œä¸­æŸ¥æ‰¾åŒ…å«æ ‡é¢˜çš„é“¾æ¥
        title_link = row.find('a', href=True)
        if title_link:
            title = title_link.get_text(strip=True)
            relative_link = title_link.get('href')

            # æ‹¼æ¥å®Œæ•´é“¾æ¥
            full_link = relative_link
            if relative_link and not relative_link.startswith(('http://', 'https://')):
                domain_base = "https://c.dadi360.com"
                full_link = f"{domain_base}{relative_link}" if relative_link.startswith('/') else f"{domain_base}/{relative_link}"

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»æ„ä¸€ä¸ªæœç´¢å…³é”®è¯
            if any(search_term in title for search_term in search_terms):
                # æå–å‘å¸–äººå’Œæ—¥æœŸä¿¡æ¯
                author_cell = row.find('td', class_='row3')
                author = ""
                if author_cell:
                    author_link = author_cell.find('a')
                    if author_link:
                        author = author_link.get_text(strip=True)
                    else:
                        author = author_cell.get_text(strip=True)

                date_cell = row.find('td', class_='row3', attrs={'nowrap': 'nowrap'})
                post_date = ""
                if date_cell:
                    # é¦–å…ˆå°è¯•æŸ¥æ‰¾spanæ ‡ç­¾ä¸­çš„æ—¥æœŸ
                    date_span = date_cell.find('span', class_='postdetails')
                    if date_span:
                        post_date = date_span.get_text(strip=True)
                    else:
                        # å¦‚æœæ²¡æœ‰spanæ ‡ç­¾ï¼Œç›´æ¥è·å–tdçš„æ–‡æœ¬å†…å®¹
                        post_date = date_cell.get_text(strip=True)
                    
                    # å¦‚æœæ—¥æœŸä¸ºç©ºï¼Œå°è¯•å…¶ä»–æ–¹å¼æŸ¥æ‰¾
                    if not post_date:
                        # æŸ¥æ‰¾æ‰€æœ‰tdï¼Œå¯»æ‰¾åŒ…å«æ—¥æœŸæ ¼å¼çš„å•å…ƒæ ¼
                        all_cells = row.find_all('td')
                        for cell in all_cells:
                            cell_text = cell.get_text(strip=True)
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼ (MM/DD/YYYY æˆ– M/D/YYYY)
                            if re.search(r'\d{1,2}/\d{1,2}/\d{4}', cell_text):
                                post_date = cell_text
                                break

                listings.append({
                    'title': title,
                    'link': full_link,
                    'author': author,
                    'date': post_date
                })
                logger.info(f"æ‰¾åˆ°åŒ¹é…æˆ¿æº: {title}")
    
    return listings

def filter_new_listings(all_listings: list[dict], existing_ids: set) -> tuple[list[dict], set]:
    """
    çº¯å‡½æ•°ï¼šä»æ‰€æœ‰æˆ¿æºä¸­è¿‡æ»¤å‡ºæ–°çš„æˆ¿æºï¼Œå¹¶æ›´æ–°å·²å‘é€IDé›†åˆã€‚
    - è¾“å…¥ï¼šæ‰€æœ‰æˆ¿æºåˆ—è¡¨, å½“å‰å·²å‘é€IDé›†åˆ
    - è¾“å‡ºï¼šæ–°æˆ¿æºåˆ—è¡¨, æ›´æ–°åçš„å·²å‘é€IDé›†åˆ
    - å‰¯ä½œç”¨ï¼šæ—  (ä½†ä¼šè¿”å›ä¸€ä¸ªæ–°çš„é›†åˆï¼Œè€Œä¸æ˜¯ä¿®æ”¹åŸé›†åˆ)
    """
    new_listings = []
    updated_ids = set(existing_ids) # å¤åˆ¶ä¸€ä»½ï¼Œç¡®ä¿ä¸ä¿®æ”¹åŸå§‹ä¼ å…¥çš„é›†åˆ

    for listing in all_listings:
        unique_id = f"{listing['title']}-{listing['link']}"
        if unique_id not in updated_ids:
            new_listings.append(listing)
            updated_ids.add(unique_id)
            logger.info(f"å‘ç°æ–°æˆ¿æº: {listing['title']}")
    return new_listings, updated_ids

def format_email_body(listings: list[dict], search_terms: list) -> tuple[str, str]:
    """
    çº¯å‡½æ•°ï¼šæ ¹æ®æˆ¿æºåˆ—è¡¨æ ¼å¼åŒ–é‚®ä»¶ä¸»é¢˜å’Œå†…å®¹ã€‚
    - è¾“å…¥ï¼šæˆ¿æºåˆ—è¡¨, æœç´¢å…³é”®è¯åˆ—è¡¨
    - è¾“å‡ºï¼š(ä¸»é¢˜å­—ç¬¦ä¸², å†…å®¹å­—ç¬¦ä¸²)
    - å‰¯ä½œç”¨ï¼šæ— 
    """
    search_terms_str = "ã€".join(search_terms[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªå…³é”®è¯ï¼Œé¿å…ä¸»é¢˜è¿‡é•¿
    subject = f"ã€æ–°å‘ç°ã€‘{search_terms_str}ç­‰æˆ¿æºé€šçŸ¥"
    
    # ç”Ÿæˆç»Ÿè®¡æ€»ç»“
    summary = summarize_listings(listings)
    
    body = f"ä½ å¥½ï¼\n\næˆ‘ä»¬å‘ç°ä»¥ä¸‹æ–°çš„æˆ¿æºä¿¡æ¯ï¼ˆåŒ¹é…å…³é”®è¯ï¼š{', '.join(search_terms)}ï¼‰ï¼š\n"
    body += summary  # æ·»åŠ ç»Ÿè®¡æ€»ç»“
    
    for idx, item in enumerate(listings):
        body += f"{idx + 1}. ğŸ“… å‘å¸ƒæ—¥æœŸ: {item.get('date', 'æœªçŸ¥')}\n"
        body += f"   ğŸ“ æ ‡é¢˜: {item['title']}\n"
        body += f"   ğŸ‘¤ å‘å¸–äºº: {item.get('author', 'æœªçŸ¥')}\n"
        body += f"   ğŸ”— é“¾æ¥: {item['link']}\n"
        if 'desc' in item and item['desc']:
            body += f"   ğŸ“„ è¯¦æƒ…: {item['desc']}\n"
        body += "   " + "â”€" * 50 + "\n"
    body += "\nè¯·å°½å¿«æŸ¥çœ‹ï¼\n\n"
    body += f"é€šçŸ¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"
    return subject, body

def send_email(email_config: dict, subject: str, body: str) -> None:
    """
    å‡½æ•°ï¼šå‘é€é‚®ä»¶é€šçŸ¥ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰å‰¯ä½œç”¨çš„å‡½æ•°ï¼ˆç½‘ç»œIOï¼‰ã€‚
    - è¾“å…¥ï¼šé‚®ä»¶é…ç½®, ä¸»é¢˜, å†…å®¹
    - è¾“å‡ºï¼šæ— 
    - å‰¯ä½œç”¨ï¼šå‘é€é‚®ä»¶
    """
    msg = MIMEText(body, 'plain', 'utf-8')
    msg['Subject'] = subject
    msg['From'] = email_config["SENDER_EMAIL"]
    msg['To'] = email_config["RECEIVER_EMAIL"]

    try:
        if email_config["SMTP_PORT"] == 465:
            server = smtplib.SMTP_SSL(email_config["SMTP_SERVER"], email_config["SMTP_PORT"])
        else:
            server = smtplib.SMTP(email_config["SMTP_SERVER"], email_config["SMTP_PORT"])
            server.starttls()
        
        server.login(email_config["SENDER_EMAIL"], email_config["SENDER_PASSWORD"])
        server.send_message(msg)
        server.quit()
        logger.success(f"é‚®ä»¶å·²å‘é€åˆ° {email_config['RECEIVER_EMAIL']}")
    except Exception as e:
        logger.error(f"å‘é€é‚®ä»¶å¤±è´¥: {e}")
        logger.error(f"è¯·æ£€æŸ¥é‚®ç®±é…ç½®ï¼šå‘ä»¶äººé‚®ç®±ã€å¯†ç ã€SMTPæœåŠ¡å™¨å’Œç«¯å£ã€‚")
        logger.error(f"å¦‚æœæ˜¯Gmailï¼Œè¯·ç¡®ä¿å·²å¼€å¯'ä¸¤æ­¥éªŒè¯'å¹¶ç”Ÿæˆ'åº”ç”¨ä¸“ç”¨å¯†ç 'ã€‚")

def fetch_listing_description(listing_url: str, headers: dict) -> str:
    html = fetch_html(listing_url, headers)
    if not html:
        return ""
    soup = BeautifulSoup(html, 'html.parser')
    postbody = soup.find('div', class_='postbody')
    if postbody:
        # Get the inner text, including <br> as newlines
        return postbody.get_text(separator='\n', strip=True)
    return ""

def summarize_listings(listings: list[dict]) -> str:
    """
    ç»Ÿè®¡æˆ¿æºä¿¡æ¯å¹¶ç”Ÿæˆæ€»ç»“
    """
    total_listings = len(listings)
    
    # ç”Ÿæˆç»Ÿè®¡æ€»ç»“
    summary = f"\nğŸ“Š æˆ¿æºç»Ÿè®¡æ€»ç»“:\n"
    summary += f"æ€»æˆ¿æºæ•°é‡: {total_listings} ä¸ª\n"
    
    # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
    date_counts = defaultdict(int)
    
    for listing in listings:
        date_str = listing.get('date', '')
        if date_str:
            # å°è¯•è§£ææ—¥æœŸ
            try:
                # å¤„ç†å¸¸è§çš„æ—¥æœŸæ ¼å¼
                date_patterns = [
                    r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
                    r'(\d{1,2})/(\d{1,2})/(\d{4})',  # 01/15/2024 æˆ– 1/15/2024
                    r'(\d{1,2})-(\d{1,2})-(\d{4})',  # 01-15-2024
                    r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2024/01/15
                ]
                
                parsed_date = None
                for pattern in date_patterns:
                    match = re.search(pattern, date_str.strip())
                    if match:
                        if len(match.group(1)) == 4:  # å¹´ä»½åœ¨å‰
                            year, month, day = match.groups()
                        else:  # æœˆä»½åœ¨å‰
                            month, day, year = match.groups()
                        
                        # ç¡®ä¿æœˆæ—¥éƒ½æ˜¯ä¸¤ä½æ•°
                        month = month.zfill(2)
                        day = day.zfill(2)
                        
                        parsed_date = datetime(int(year), int(month), int(day))
                        break
                
                if parsed_date:
                    # æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD
                    formatted_date = parsed_date.strftime('%Y-%m-%d')
                    date_counts[formatted_date] += 1
                else:
                    # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
                    date_counts[date_str] += 1
                    
            except Exception as e:
                logger.warning(f"è§£ææ—¥æœŸå¤±è´¥ '{date_str}': {e}")
                date_counts[date_str] += 1
        else:
            # æ²¡æœ‰æ—¥æœŸçš„å½’ä¸º"æœªçŸ¥æ—¥æœŸ"
            date_counts["æœªçŸ¥æ—¥æœŸ"] += 1
    
    # æŒ‰å…³é”®è¯åˆ†ç±»ç»Ÿè®¡
    keyword_counts = {}
    rental_config = CONFIG.get("rental", {})
    search_terms = rental_config.get("search_terms", ["2æˆ¿ä¸€å…", "ä¸¤æˆ¿ä¸€å…", "2å§ä¸€å…", "ä¸¤å§ä¸€å…", "2å®¤1å…", "ä¸¤å®¤ä¸€å…", "2æˆ¿1å…", "ä¸¤æˆ¿1å…", "2å§1å…", "ä¸¤å§1å…"])
    
    for listing in listings:
        title = listing.get('title', '').lower()
        for term in search_terms:
            if term.lower() in title:
                keyword_counts[term] = keyword_counts.get(term, 0) + 1
                break  # åªåŒ¹é…ç¬¬ä¸€ä¸ªå…³é”®è¯
    
    # æ·»åŠ æ—¥æœŸåˆ†å¸ƒç»Ÿè®¡
    if date_counts:
        summary += f"æŒ‰æ—¥æœŸåˆ†å¸ƒ:\n"
        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœªçŸ¥æ—¥æœŸæ”¾åœ¨æœ€åï¼‰
        sorted_dates = sorted(
            date_counts.items(),
            key=lambda x: (x[0] == "æœªçŸ¥æ—¥æœŸ", x[0])
        )
        
        for date, count in sorted_dates:
            summary += f"  â€¢ {date}: {count} ä¸ªæˆ¿æº\n"
    
    # æ·»åŠ å…³é”®è¯åˆ†å¸ƒç»Ÿè®¡
    if keyword_counts:
        summary += f"æŒ‰å…³é”®è¯åˆ†å¸ƒ:\n"
        for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True):
            summary += f"  â€¢ {keyword}: {count} ä¸ªæˆ¿æº\n"
    else:
        summary += f"  â€¢ æš‚æ— å…³é”®è¯åŒ¹é…ä¿¡æ¯\n"
    
    summary += f"\n"
    return summary

# --- ä¸»åè°ƒå‡½æ•° ---

def scrape_and_notify_job(config: dict, current_sent_ids: set) -> set:
    """
    ä¸»åè°ƒå‡½æ•°ï¼šæ‰§è¡ŒæŠ“å–ã€ç­›é€‰å’Œé€šçŸ¥çš„æ•´ä¸ªæµç¨‹ã€‚
    - è¾“å…¥ï¼šé…ç½®å­—å…¸, å½“å‰å·²å‘é€IDé›†åˆ
    - è¾“å‡ºï¼šæ›´æ–°åçš„å·²å‘é€IDé›†åˆ (æ–¹ä¾¿å¤–éƒ¨æŒä¹…åŒ–)
    - å‰¯ä½œç”¨ï¼šæ‰§è¡Œç½‘ç»œè¯·æ±‚, å‘é€é‚®ä»¶
    """
    logger.info(f"--- ä»»åŠ¡å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')} ---")
    all_raw_listings = []

    # ä»rentalé…ç½®ä¸­è·å–å‚æ•°
    rental_config = config.get("rental", {})
    target_url_base = rental_config.get("target_url_base", "https://c.dadi360.com/c/forums/show//87.page")
    num_pages_to_scrape = rental_config.get("num_pages_to_scrape", 5)
    search_terms = rental_config.get("search_terms", ["2æˆ¿ä¸€å…", "ä¸¤æˆ¿ä¸€å…", "2å§ä¸€å…", "ä¸¤å§ä¸€å…", "2å®¤1å…", "ä¸¤å®¤ä¸€å…", "2æˆ¿1å…", "ä¸¤æˆ¿1å…", "2å§1å…", "ä¸¤å§1å…"])

    for page_num in range(1, num_pages_to_scrape + 1):
        # æ ¹æ®é¡µç è®¡ç®—URLè·¯å¾„
        if page_num == 1:
            page_url = target_url_base
        else:
            # æ¯é¡µ90ä¸ªå¸–å­ï¼Œæ‰€ä»¥ç¬¬2é¡µæ˜¯90ï¼Œç¬¬3é¡µæ˜¯180ï¼Œç¬¬4é¡µæ˜¯270
            offset = (page_num - 1) * 90
            page_url = f"https://c.dadi360.com/c/forums/show/{offset}/87.page"
        
        html_content = fetch_html(page_url, config["HEADERS"])
        if html_content:
            # parse_html_for_listings æ˜¯çº¯å‡½æ•°ï¼Œè¿”å›å½“å‰é¡µæ‰€æœ‰æˆ¿æº
            listings_on_page = parse_html_for_listings(
                html_content, target_url_base, search_terms
            )
            all_raw_listings.extend(listings_on_page)
        
        time.sleep(2) # ç¤¼è²Œæ€§å»¶è¿Ÿ

    # è¿‡æ»¤å‡ºæ–°æˆ¿æºå¹¶æ›´æ–°å·²å‘é€IDé›†åˆ
    new_listings, updated_sent_ids = filter_new_listings(all_raw_listings, current_sent_ids)

    # â­â­â­ Fetch and attach description for each new listing
    for listing in new_listings:
        listing['desc'] = fetch_listing_description(listing['link'], config["HEADERS"])

    # æ ¹æ®å‘å¸ƒæ—¥æœŸæ’åºï¼Œæœ€æ–°çš„æ’åœ¨å‰é¢
    try:
        def parse_date(date_str):
            """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å›å¯æ¯”è¾ƒçš„æ—¥æœŸå¯¹è±¡"""
            if not date_str:
                return None
            try:
                # å¤„ç†å¸¸è§çš„æ—¥æœŸæ ¼å¼
                date_patterns = [
                    r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
                    r'(\d{1,2})/(\d{1,2})/(\d{4})',  # 01/15/2024 æˆ– 1/15/2024
                    r'(\d{1,2})-(\d{1,2})-(\d{4})',  # 01-15-2024
                    r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2024/01/15
                ]
                
                for pattern in date_patterns:
                    match = re.search(pattern, date_str.strip())
                    if match:
                        if len(match.group(1)) == 4:  # å¹´ä»½åœ¨å‰
                            year, month, day = match.groups()
                        else:  # æœˆä»½åœ¨å‰
                            month, day, year = match.groups()
                        
                        # ç¡®ä¿æœˆæ—¥éƒ½æ˜¯ä¸¤ä½æ•°
                        month = month.zfill(2)
                        day = day.zfill(2)
                        
                        return datetime(int(year), int(month), int(day))
                
                # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›None
                return None
            except:
                return None
        
        # æŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        def sort_key(listing):
            """æ’åºé”®å‡½æ•°ï¼Œå¤„ç†Noneæ—¥æœŸ"""
            date_obj = parse_date(listing.get('date', ''))
            # å¦‚æœæ—¥æœŸä¸ºNoneï¼Œè¿”å›ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸä½œä¸ºfallback
            if date_obj is None:
                from datetime import datetime
                return (datetime(1900, 1, 1), 0)  # å¾ˆæ—©çš„æ—¥æœŸï¼Œæ’åœ¨æœ€å
            return (date_obj, 0)  # æ­£å¸¸æ—¥æœŸ
        
        new_listings.sort(key=sort_key, reverse=True)
        
        # ç§»é™¤æ— æ³•è§£ææ—¥æœŸçš„é¡¹ç›®ï¼ˆæ”¾åœ¨æœ€åï¼‰
        valid_listings = [listing for listing in new_listings if parse_date(listing.get('date', '')) is not None]
        invalid_listings = [listing for listing in new_listings if parse_date(listing.get('date', '')) is None]
        new_listings = valid_listings + invalid_listings
    except Exception as e:
        logger.error(f"æ’åºæˆ¿æºä¿¡æ¯æ—¶å‡ºé”™: {e}")

    if new_listings:
        # æ ¼å¼åŒ–é‚®ä»¶å†…å®¹ï¼Œä¹Ÿæ˜¯çº¯å‡½æ•°
        subject, body = format_email_body(new_listings, search_terms)
        
        # ç”Ÿæˆå¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯
        summary = summarize_listings(new_listings)
        logger.info(f"ğŸ“Š å‘ç°æ–°æˆ¿æºç»Ÿè®¡: {summary.strip()}")
        
        # æ‰“å°é‚®ä»¶å†…å®¹é¢„è§ˆ
        logger.info(f"\nğŸ“§ é‚®ä»¶é¢„è§ˆ:")
        logger.info(f"ä¸»é¢˜: {subject}")
        logger.info(f"æ”¶ä»¶äºº: {config['EMAIL']['RECEIVER_EMAIL']}")
        logger.info(f"å†…å®¹:\n{body}")
        logger.info(f"--- é‚®ä»¶é¢„è§ˆç»“æŸ ---\n")
        
        # å‘é€é‚®ä»¶æ˜¯æœ‰å‰¯ä½œç”¨çš„æ“ä½œ
        send_email(config["EMAIL"], subject, body)
    else:
        search_terms_str = "ã€".join(search_terms[:3])
        logger.info(f"æ­¤æ¬¡ä»»åŠ¡æœªå‘ç°æ–°çš„ '{search_terms_str}ç­‰' æˆ¿æºã€‚")
    
    logger.info(f"--- ä»»åŠ¡ç»“æŸ: {time.strftime('%Y-%m-%d %H:%M:%S')} ---\n")
    return updated_sent_ids # è¿”å›æ›´æ–°åçš„ ID é›†åˆ


# --- è°ƒåº¦å™¨è®¾ç½®å’Œä¸»å…¥å£ ---

if __name__ == "__main__":
    logger.info("æˆ¿æºç›‘æ§è„šæœ¬å·²å¯åŠ¨ã€‚")
    rental_config = CONFIG.get("rental", {})
    search_terms = rental_config.get("search_terms", ["2æˆ¿ä¸€å…", "ä¸¤æˆ¿ä¸€å…", "2å§ä¸€å…", "ä¸¤å§ä¸€å…", "2å®¤1å…", "ä¸¤å®¤ä¸€å…", "2æˆ¿1å…", "ä¸¤æˆ¿1å…", "2å§1å…", "ä¸¤å§1å…"])
    target_url_base = rental_config.get("target_url_base", "https://c.dadi360.com/c/forums/show//87.page")
    num_pages_to_scrape = rental_config.get("num_pages_to_scrape", 5)
    send_interval_minutes = rental_config.get("send_interval_minutes", 10)
    
    search_terms_str = "ã€".join(search_terms)
    logger.info(f"å°†æ¯{send_interval_minutes}åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ {target_url_base} çš„å‰ {num_pages_to_scrape} é¡µï¼ŒæŸ¥æ‰¾: {search_terms_str}")
    logger.info(f"é€šçŸ¥å°†å‘é€åˆ°: {CONFIG['EMAIL']['RECEIVER_EMAIL']}")
    logger.info("æŒ‰ Ctrl+C åœæ­¢è„šæœ¬ã€‚")

    # å®šä¹‰ä¸€ä¸ªåŒ…è£…å‡½æ•°ï¼Œä»¥ä¾¿å°†å¤–éƒ¨çŠ¶æ€ (sent_listing_ids) ä¼ é€’ç»™è°ƒåº¦ä»»åŠ¡
    def scheduled_task():
        global _sent_listing_ids # å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
        _sent_listing_ids = scrape_and_notify_job(CONFIG, _sent_listing_ids)
        save_sent_ids(SENT_IDS_FILE, _sent_listing_ids) # æ¯æ¬¡ä»»åŠ¡ç»“æŸåä¿å­˜çŠ¶æ€

    # ç«‹å³æ‰§è¡Œä¸€æ¬¡ï¼Œç„¶åæ¯æŒ‡å®šåˆ†é’Ÿæ‰§è¡Œ
    scheduled_task() # é¦–æ¬¡è¿è¡Œ
    schedule.every(send_interval_minutes).minutes.do(scheduled_task)

    while True:
        schedule.run_pending()
        time.sleep(1)