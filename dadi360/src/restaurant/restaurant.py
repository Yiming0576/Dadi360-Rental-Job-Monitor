import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
import time
import os
import json
import urllib3
import re
import sys
from loguru import logger
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from scheduler_util import Scheduler

# é…ç½®loguruæ—¥å¿—
logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨

# è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
LOGS_DIR = os.path.join(PROJECT_ROOT, "logs")

# ä¿¡æ¯æ—¥å¿— - è®°å½•æ‰€æœ‰INFOåŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
logger.add(
    os.path.join(LOGS_DIR, "restaurant_jobs_info.log"),
    rotation="10 MB",
    retention="30 days",
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    encoding="utf-8",
    filter=lambda record: record["level"].name in ["INFO", "SUCCESS", "WARNING"]
)

# é”™è¯¯æ—¥å¿— - åªè®°å½•ERRORåŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
logger.add(
    os.path.join(LOGS_DIR, "restaurant_jobs_error.log"),
    rotation="5 MB",
    retention="60 days",
    level="ERROR",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
    encoding="utf-8",
    filter=lambda record: record["level"].name in ["ERROR", "CRITICAL"]
)

# æ§åˆ¶å°è¾“å‡º - æ˜¾ç¤ºæ‰€æœ‰INFOåŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
logger.add(
    sys.stdout,
    level="INFO",
    format="{time:HH:mm:ss} | {level} | {message}",
    colorize=True
)

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs(LOGS_DIR, exist_ok=True)

# æŠ‘åˆ¶SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®ä¿¡æ¯ä» config.json åŠ è½½ ---
CONFIG_PATH = os.path.join(os.path.dirname(__file__), '../../config.json')
with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
    CONFIG = json.load(f)

# --- è¾…åŠ©å‡½æ•°ï¼šæŒä¹…åŒ–å·²å‘é€ID ---
def load_sent_ids(file_path):
    """ä»æ–‡ä»¶ä¸­åŠ è½½å·²å‘é€çš„æ‹›è˜ä¿¡æ¯IDé›†åˆã€‚"""
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        except (json.JSONDecodeError, IOError) as e:
            logger.error(f"åŠ è½½å·²å‘é€IDæ–‡ä»¶å¤±è´¥ {file_path}: {e}. åˆå§‹åŒ–ä¸ºç©ºé›†åˆã€‚")
            return set()
    return set()

def save_sent_ids(file_path, ids_set):
    """å°†å·²å‘é€çš„æ‹›è˜ä¿¡æ¯IDé›†åˆä¿å­˜åˆ°æ–‡ä»¶ã€‚"""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(list(ids_set), f, ensure_ascii=False, indent=4)
    except IOError as e:
        logger.error(f"ä¿å­˜å·²å‘é€IDæ–‡ä»¶å¤±è´¥ {file_path}: {e}")

# åˆå§‹åŒ–å·²å‘é€IDé›†åˆ
SENT_IDS_FILE = os.path.join(os.path.dirname(__file__), "sent_restaurant_ids.json")
_sent_restaurant_ids = load_sent_ids(SENT_IDS_FILE)

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def fetch_html(url: str, headers: dict) -> str | None:
    """
    çº¯å‡½æ•°ï¼šæ ¹æ®URLå’Œè¯·æ±‚å¤´è·å–ç½‘é¡µçš„HTMLå†…å®¹ã€‚
    """
    try:
        logger.info(f"æ­£åœ¨è¯·æ±‚: {url}")
        response = requests.get(url, headers=headers, timeout=15, verify=False)
        response.raise_for_status()
        return response.text
    except requests.exceptions.RequestException as e:
        logger.error(f"è¯·æ±‚ {url} å¤±è´¥: {e}")
        return None

def parse_html_for_restaurant_jobs(html_content: str, base_url: str, search_terms: list) -> list[dict]:
    """
    çº¯å‡½æ•°ï¼šè§£æHTMLå†…å®¹ï¼Œæå–ç¬¦åˆæ¡ä»¶çš„é¤å…æ‹›è˜ä¿¡æ¯ã€‚
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    jobs = []

    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ‹›è˜ä¿¡æ¯çš„è¡¨æ ¼è¡Œ
    topic_rows = soup.find_all('tr', class_='bg_small_yellow')

    for row in topic_rows:
        # åœ¨æ¯è¡Œä¸­æŸ¥æ‰¾åŒ…å«æ ‡é¢˜çš„é“¾æ¥
        title_link = row.find('a', href=True)
        if title_link:
            title = title_link.get_text(strip=True)
            relative_link = title_link.get('href')

            # æ‹¼æ¥å®Œæ•´é“¾æ¥
            full_link = relative_link
            if relative_link and not relative_link.startswith(('http://', 'https://')):
                domain_base = "https://c.dadi360.com"
                full_link = f"{domain_base}{relative_link}" if relative_link.startswith('/') else f"{domain_base}/{relative_link}"

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»æ„ä¸€ä¸ªæœç´¢å…³é”®è¯
            if any(search_term in title for search_term in search_terms):
                # æå–å‘å¸–äººå’Œæ—¥æœŸä¿¡æ¯
                author_cell = row.find('td', class_='row3')
                author = ""
                if author_cell:
                    author_link = author_cell.find('a')
                    if author_link:
                        author = author_link.get_text(strip=True)
                    else:
                        author = author_cell.get_text(strip=True)

                date_cell = row.find('td', class_='row3', attrs={'nowrap': 'nowrap'})
                post_date = ""
                if date_cell:
                    # é¦–å…ˆå°è¯•æŸ¥æ‰¾spanæ ‡ç­¾ä¸­çš„æ—¥æœŸ
                    date_span = date_cell.find('span', class_='postdetails')
                    if date_span:
                        post_date = date_span.get_text(strip=True)
                    else:
                        # å¦‚æœæ²¡æœ‰spanæ ‡ç­¾ï¼Œç›´æ¥è·å–tdçš„æ–‡æœ¬å†…å®¹
                        post_date = date_cell.get_text(strip=True)
                    
                    # å¦‚æœæ—¥æœŸä¸ºç©ºï¼Œå°è¯•å…¶ä»–æ–¹å¼æŸ¥æ‰¾
                    if not post_date:
                        # æŸ¥æ‰¾æ‰€æœ‰tdï¼Œå¯»æ‰¾åŒ…å«æ—¥æœŸæ ¼å¼çš„å•å…ƒæ ¼
                        all_cells = row.find_all('td')
                        for cell in all_cells:
                            cell_text = cell.get_text(strip=True)
                            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ—¥æœŸæ ¼å¼ (MM/DD/YYYY æˆ– M/D/YYYY)
                            if re.search(r'\d{1,2}/\d{1,2}/\d{4}', cell_text):
                                post_date = cell_text
                                break

                jobs.append({
                    'title': title,
                    'link': full_link,
                    'author': author,
                    'date': post_date
                })
                logger.info(f"æ‰¾åˆ°åŒ¹é…æ‹›è˜ä¿¡æ¯: {title}")
    
    return jobs

def filter_new_jobs(all_jobs: list[dict], existing_ids: set) -> tuple[list[dict], set]:
    """
    çº¯å‡½æ•°ï¼šä»æ‰€æœ‰æ‹›è˜ä¿¡æ¯ä¸­è¿‡æ»¤å‡ºæ–°çš„ä¿¡æ¯ï¼Œå¹¶æ›´æ–°å·²å‘é€IDé›†åˆã€‚
    """
    new_jobs = []
    updated_ids = set(existing_ids)

    for job in all_jobs:
        unique_id = f"{job['title']}-{job['link']}"
        if unique_id not in updated_ids:
            new_jobs.append(job)
            updated_ids.add(unique_id)
            logger.info(f"å‘ç°æ–°æ‹›è˜ä¿¡æ¯: {job['title']}")
    return new_jobs, updated_ids

def format_email_body(jobs: list[dict], search_terms: list) -> tuple[str, str]:
    """
    çº¯å‡½æ•°ï¼šæ ¹æ®æ‹›è˜ä¿¡æ¯åˆ—è¡¨æ ¼å¼åŒ–é‚®ä»¶ä¸»é¢˜å’Œå†…å®¹ã€‚
    """
    search_terms_str = "ã€".join(search_terms[:3])
    subject = f"ã€é¤å…æ‹›è˜ã€‘{search_terms_str}ç­‰æ‹›è˜ä¿¡æ¯é€šçŸ¥"
    
    # ç”Ÿæˆç»Ÿè®¡æ€»ç»“
    summary = summarize_jobs_by_date(jobs)
    
    body = f"ä½ å¥½ï¼\n\næˆ‘ä»¬å‘ç°ä»¥ä¸‹æ–°çš„é¤å…æ‹›è˜ä¿¡æ¯ï¼ˆåŒ¹é…å…³é”®è¯ï¼š{', '.join(search_terms)}ï¼‰ï¼š\n"
    body += summary  # æ·»åŠ ç»Ÿè®¡æ€»ç»“
    
    for idx, job in enumerate(jobs):
        body += f"{idx + 1}. ğŸ“… å‘å¸ƒæ—¥æœŸ: {job['date']}\n"
        body += f"   ğŸ“ æ ‡é¢˜: {job['title']}\n"
        body += f"   ğŸ‘¤ å‘å¸–äºº: {job['author']}\n"
        body += f"   ğŸ”— é“¾æ¥: {job['link']}\n"
        if 'desc' in job and job['desc']:
            body += f"   ğŸ“„ è¯¦æƒ…: {job['desc']}\n"
        body += "   " + "â”€" * 50 + "\n"
    
    body += "\nè¯·å°½å¿«æŸ¥çœ‹ï¼\n\n"
    body += f"é€šçŸ¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"
    return subject, body

def send_email(email_config: dict, subject: str, body: str) -> None:
    """
    å‡½æ•°ï¼šå‘é€é‚®ä»¶é€šçŸ¥ã€‚
    """
    msg = MIMEText(body, 'plain', 'utf-8')
    msg['Subject'] = subject
    msg['From'] = email_config["SENDER_EMAIL"]
    msg['To'] = email_config["RECEIVER_EMAIL"]

    try:
        if email_config["SMTP_PORT"] == 465:
            server = smtplib.SMTP_SSL(email_config["SMTP_SERVER"], email_config["SMTP_PORT"])
        else:
            server = smtplib.SMTP(email_config["SMTP_SERVER"], email_config["SMTP_PORT"])
            server.starttls()
        
        server.login(email_config["SENDER_EMAIL"], email_config["SENDER_PASSWORD"])
        server.send_message(msg)
        server.quit()
        logger.success(f"é‚®ä»¶å·²å‘é€åˆ° {email_config['RECEIVER_EMAIL']}")
    except Exception as e:
        logger.error(f"å‘é€é‚®ä»¶å¤±è´¥: {e}")
        logger.error(f"è¯·æ£€æŸ¥é‚®ç®±é…ç½®ï¼šå‘ä»¶äººé‚®ç®±ã€å¯†ç ã€SMTPæœåŠ¡å™¨å’Œç«¯å£ã€‚")

def fetch_job_description(job_url: str, headers: dict) -> str:
    """è·å–æ‹›è˜ä¿¡æ¯çš„è¯¦ç»†æè¿°"""
    html = fetch_html(job_url, headers)
    if not html:
        return ""
    soup = BeautifulSoup(html, 'html.parser')
    postbody = soup.find('div', class_='postbody')
    if postbody:
        return postbody.get_text(separator='\n', strip=True)
    return ""

def summarize_jobs_by_date(jobs: list[dict]) -> str:
    """
    æŒ‰æ—¥æœŸç»Ÿè®¡å·¥ä½œæ•°é‡å¹¶ç”Ÿæˆæ€»ç»“
    """
    from collections import defaultdict
    from datetime import datetime
    import re
    
    # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
    date_counts = defaultdict(int)
    total_jobs = len(jobs)
    
    for job in jobs:
        date_str = job.get('date', '')
        if date_str:
            # å°è¯•è§£ææ—¥æœŸ
            try:
                # å¤„ç†å¸¸è§çš„æ—¥æœŸæ ¼å¼
                date_patterns = [
                    r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
                    r'(\d{1,2})/(\d{1,2})/(\d{4})',  # 01/15/2024 æˆ– 1/15/2024
                    r'(\d{1,2})-(\d{1,2})-(\d{4})',  # 01-15-2024
                    r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2024/01/15
                ]
                
                parsed_date = None
                for pattern in date_patterns:
                    match = re.search(pattern, date_str.strip())
                    if match:
                        if len(match.group(1)) == 4:  # å¹´ä»½åœ¨å‰
                            year, month, day = match.groups()
                        else:  # æœˆä»½åœ¨å‰
                            month, day, year = match.groups()
                        
                        # ç¡®ä¿æœˆæ—¥éƒ½æ˜¯ä¸¤ä½æ•°
                        month = month.zfill(2)
                        day = day.zfill(2)
                        
                        parsed_date = datetime(int(year), int(month), int(day))
                        break
                
                if parsed_date:
                    # æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD
                    formatted_date = parsed_date.strftime('%Y-%m-%d')
                    date_counts[formatted_date] += 1
                else:
                    # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
                    date_counts[date_str] += 1
                    
            except Exception as e:
                logger.warning(f"è§£ææ—¥æœŸå¤±è´¥ '{date_str}': {e}")
                date_counts[date_str] += 1
        else:
            # æ²¡æœ‰æ—¥æœŸçš„å½’ä¸º"æœªçŸ¥æ—¥æœŸ"
            date_counts["æœªçŸ¥æ—¥æœŸ"] += 1
    
    # ç”Ÿæˆç»Ÿè®¡æ€»ç»“
    summary = f"\nğŸ“Š å·¥ä½œç»Ÿè®¡æ€»ç»“:\n"
    summary += f"æ€»å·¥ä½œæ•°é‡: {total_jobs} ä¸ª\n"
    summary += f"æŒ‰æ—¥æœŸåˆ†å¸ƒ:\n"
    
    if date_counts:
        # æŒ‰æ—¥æœŸæ’åºï¼ˆæœªçŸ¥æ—¥æœŸæ”¾åœ¨æœ€åï¼‰
        sorted_dates = sorted(
            date_counts.items(),
            key=lambda x: (x[0] == "æœªçŸ¥æ—¥æœŸ", x[0])
        )
        
        for date, count in sorted_dates:
            summary += f"  â€¢ {date}: {count} ä¸ªå·¥ä½œ\n"
    else:
        summary += f"  â€¢ æš‚æ— æ—¥æœŸä¿¡æ¯\n"
    
    summary += f"\n"
    return summary

# --- ä¸»åè°ƒå‡½æ•° ---

def scrape_and_notify_restaurant_jobs(config: dict, current_sent_ids: set) -> set:
    """
    ä¸»åè°ƒå‡½æ•°ï¼šæ‰§è¡Œé¤å…æ‹›è˜ä¿¡æ¯æŠ“å–ã€ç­›é€‰å’Œé€šçŸ¥çš„æ•´ä¸ªæµç¨‹ã€‚
    """
    try:
        logger.info(f"--- é¤å…æ‹›è˜ç›‘æ§ä»»åŠ¡å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')} ---")
        all_raw_jobs = []

        # é¤å…æ‹›è˜é¡µé¢çš„URLæ¨¡å¼
        restaurant_urls = [
            "https://c.dadi360.com/c/forums/show//57.page",  # ç¬¬1é¡µ
            "https://c.dadi360.com/c/forums/show/90/57.page",  # ç¬¬2é¡µ
            "https://c.dadi360.com/c/forums/show/180/57.page",  # ç¬¬3é¡µ
            "https://c.dadi360.com/c/forums/show/270/57.page",  # ç¬¬4é¡µ
            "https://c.dadi360.com/c/forums/show/360/57.page"   # ç¬¬5é¡µ
        ]

        for page_num, page_url in enumerate(restaurant_urls, 1):
            try:
                logger.info(f"æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
                html_content = fetch_html(page_url, config["HEADERS"])
                if html_content:
                    # ä½¿ç”¨é¤å…æ‹›è˜å…³é”®è¯
                    restaurant_keywords = config.get("restaurant_jobs", {}).get("keywords", ["é¤å…", "é¤é¦†", "å¨å¸ˆ", "ä¼å°", "æ”¶é“¶", "æ‰“æ‚", "æ²¹é”…", "å¯¿å¸", "é“æ¿", "å¤–å–"])
                    jobs_on_page = parse_html_for_restaurant_jobs(
                        html_content, page_url, restaurant_keywords
                    )
                    all_raw_jobs.extend(jobs_on_page)
                else:
                    logger.warning(f"ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥ï¼Œè·³è¿‡")
            except Exception as e:
                logger.error(f"æŠ“å–ç¬¬ {page_num} é¡µæ—¶å‡ºé”™: {e}")
                continue
            
            time.sleep(2)  # ç¤¼è²Œæ€§å»¶è¿Ÿ

        # è¿‡æ»¤å‡ºæ–°æ‹›è˜ä¿¡æ¯å¹¶æ›´æ–°å·²å‘é€IDé›†åˆ
        try:
            new_jobs, updated_sent_ids = filter_new_jobs(all_raw_jobs, current_sent_ids)
        except Exception as e:
            logger.error(f"è¿‡æ»¤æ‹›è˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return current_sent_ids

        # è·å–æ¯ä¸ªæ–°æ‹›è˜ä¿¡æ¯çš„è¯¦ç»†æè¿°
        for job in new_jobs:
            try:
                job['desc'] = fetch_job_description(job['link'], config["HEADERS"])
            except Exception as e:
                logger.warning(f"è·å–æ‹›è˜è¯¦æƒ…å¤±è´¥: {e}")
                job['desc'] = "è¯¦æƒ…è·å–å¤±è´¥"

        # æ ¹æ®å‘å¸ƒæ—¥æœŸæ’åºï¼Œæœ€æ–°çš„æ’åœ¨å‰é¢
        try:
            def parse_date(date_str):
                """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å›å¯æ¯”è¾ƒçš„æ—¥æœŸå¯¹è±¡"""
                if not date_str:
                    return None
                try:
                    # å¤„ç†å¸¸è§çš„æ—¥æœŸæ ¼å¼
                    from datetime import datetime
                    
                    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
                    date_str = date_str.strip()
                    
                    # å°è¯•è§£æä¸åŒçš„æ—¥æœŸæ ¼å¼
                    date_patterns = [
                        r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
                        r'(\d{1,2})/(\d{1,2})/(\d{4})',  # 01/15/2024 æˆ– 1/15/2024
                        r'(\d{1,2})-(\d{1,2})-(\d{4})',  # 01-15-2024
                        r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2024/01/15
                    ]
                    
                    for pattern in date_patterns:
                        match = re.search(pattern, date_str)
                        if match:
                            if len(match.group(1)) == 4:  # å¹´ä»½åœ¨å‰
                                year, month, day = match.groups()
                            else:  # æœˆä»½åœ¨å‰
                                month, day, year = match.groups()
                            
                            # ç¡®ä¿æœˆæ—¥éƒ½æ˜¯ä¸¤ä½æ•°
                            month = month.zfill(2)
                            day = day.zfill(2)
                            
                            return datetime(int(year), int(month), int(day))
                    
                    # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›None
                    return None
                except:
                    return None
            
            # æŒ‰æ—¥æœŸæ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            def sort_key(job):
                """æ’åºé”®å‡½æ•°ï¼Œå¤„ç†Noneæ—¥æœŸ"""
                date_obj = parse_date(job.get('date', ''))
                # å¦‚æœæ—¥æœŸä¸ºNoneï¼Œè¿”å›ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸä½œä¸ºfallback
                if date_obj is None:
                    from datetime import datetime
                    return (datetime(1900, 1, 1), 0)  # å¾ˆæ—©çš„æ—¥æœŸï¼Œæ’åœ¨æœ€å
                return (date_obj, 0)  # æ­£å¸¸æ—¥æœŸ
            
            new_jobs.sort(key=sort_key, reverse=True)
            
            # ç§»é™¤æ— æ³•è§£ææ—¥æœŸçš„é¡¹ç›®ï¼ˆæ”¾åœ¨æœ€åï¼‰
            valid_jobs = [job for job in new_jobs if parse_date(job.get('date', '')) is not None]
            invalid_jobs = [job for job in new_jobs if parse_date(job.get('date', '')) is None]
            new_jobs = valid_jobs + invalid_jobs
        except Exception as e:
            logger.error(f"æ’åºæ‹›è˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")

        if new_jobs:
            # æ ¼å¼åŒ–é‚®ä»¶å†…å®¹
            try:
                restaurant_keywords = config.get("restaurant_jobs", {}).get("keywords", ["é¤å…", "é¤é¦†", "å¨å¸ˆ", "ä¼å°", "æ”¶é“¶", "æ‰“æ‚", "æ²¹é”…", "å¯¿å¸", "é“æ¿", "å¤–å–"])
                
                # ç”Ÿæˆå¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯
                summary = summarize_jobs_by_date(new_jobs)
                logger.info(f"ğŸ“Š å‘ç°æ–°å·¥ä½œç»Ÿè®¡: {summary.strip()}")
                
                subject, body = format_email_body(new_jobs, restaurant_keywords)
                
                # æ‰“å°é‚®ä»¶å†…å®¹é¢„è§ˆ
                logger.info(f"\nğŸ“§ é‚®ä»¶é¢„è§ˆ:")
                logger.info(f"ä¸»é¢˜: {subject}")
                logger.info(f"æ”¶ä»¶äºº: {config['EMAIL']['RECEIVER_EMAIL']}")
                logger.info(f"å†…å®¹:\n{body}")
                logger.info(f"--- é‚®ä»¶é¢„è§ˆç»“æŸ ---\n")
                
                # å‘é€é‚®ä»¶
                send_email(config["EMAIL"], subject, body)
            except Exception as e:
                logger.error(f"å‘é€é‚®ä»¶æ—¶å‡ºé”™: {e}")
        else:
            try:
                restaurant_keywords = config.get("restaurant_jobs", {}).get("keywords", ["é¤å…", "é¤é¦†", "å¨å¸ˆ", "ä¼å°", "æ”¶é“¶", "æ‰“æ‚", "æ²¹é”…", "å¯¿å¸", "é“æ¿", "å¤–å–"])
                search_terms_str = "ã€".join(restaurant_keywords[:3])
                logger.info(f"æ­¤æ¬¡ä»»åŠ¡æœªå‘ç°æ–°çš„ '{search_terms_str}ç­‰' æ‹›è˜ä¿¡æ¯ã€‚")
            except Exception as e:
                logger.error(f"å¤„ç†æ— æ–°ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        logger.info(f"--- é¤å…æ‹›è˜ç›‘æ§ä»»åŠ¡ç»“æŸ: {time.strftime('%Y-%m-%d %H:%M:%S')} ---\n")
        return updated_sent_ids
        
    except Exception as e:
        logger.error(f"é¤å…æ‹›è˜ç›‘æ§ä»»åŠ¡å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        logger.error(f"ç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œç­‰å¾…ä¸‹æ¬¡è°ƒåº¦...")
        return current_sent_ids

# --- è°ƒåº¦å™¨è®¾ç½®å’Œä¸»å…¥å£ ---

def scheduled_task():
    """è°ƒåº¦ä»»åŠ¡å‡½æ•°"""
    global _sent_restaurant_ids
    try:
        _sent_restaurant_ids = scrape_and_notify_restaurant_jobs(CONFIG, _sent_restaurant_ids)
        save_sent_ids(SENT_IDS_FILE, _sent_restaurant_ids)
    except Exception as e:
        logger.error(f"è°ƒåº¦ä»»åŠ¡å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        logger.error(f"ç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œç­‰å¾…ä¸‹æ¬¡è°ƒåº¦...")
        # å³ä½¿å‡ºé”™ä¹Ÿè¦ä¿å­˜å½“å‰çŠ¶æ€
        try:
            save_sent_ids(SENT_IDS_FILE, _sent_restaurant_ids)
        except Exception as save_error:
            logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {save_error}")

if __name__ == "__main__":
    try:
        logger.info("é¤å…æ‹›è˜ç›‘æ§è„šæœ¬å·²å¯åŠ¨ã€‚")
        restaurant_config = CONFIG.get("restaurant_jobs", {})
        keywords = restaurant_config.get("keywords", ["é¤å…", "é¤é¦†", "å¨å¸ˆ", "ä¼å°", "æ”¶é“¶", "æ‰“æ‚", "æ²¹é”…", "å¯¿å¸", "é“æ¿", "å¤–å–"])
        keywords_str = "ã€".join(keywords[:5])
        interval_seconds = restaurant_config.get("send_interval_seconds", 172800)  # é»˜è®¤2å¤©
        interval_hours = interval_seconds // 3600  # è½¬æ¢ä¸ºå°æ—¶
        
        logger.info(f"å°†æ¯{interval_hours}å°æ—¶æ£€æŸ¥ä¸€æ¬¡é¤å…æ‹›è˜é¡µé¢çš„å‰5é¡µï¼ŒæŸ¥æ‰¾: {keywords_str}")
        logger.info(f"é€šçŸ¥å°†å‘é€åˆ°: {CONFIG['EMAIL']['RECEIVER_EMAIL']}")
        logger.info("æŒ‰ Ctrl+C åœæ­¢è„šæœ¬ã€‚")

        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = Scheduler()
        
        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        scheduled_task()
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        scheduler.every(interval_seconds, scheduled_task)
        
        # å¯åŠ¨è°ƒåº¦å™¨
        scheduler.start()
        
        try:
            # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œ
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            logger.info("\næ­£åœ¨åœæ­¢é¤å…æ‹›è˜ç›‘æ§è„šæœ¬...")
            scheduler.stop()
            logger.info("è„šæœ¬å·²åœæ­¢ã€‚")
            
    except Exception as e:
        logger.error(f"ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
        logger.error("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç›¸å…³è®¾ç½®ã€‚") 