import requests
from bs4 import BeautifulSoup
import smtplib
from email.mime.text import MIMEText
import time
import os
import json
import urllib3
import re
import sys
from loguru import logger
from abc import ABC, abstractmethod
from typing import List, Dict, Set, Tuple, Optional
from datetime import datetime
from collections import defaultdict

# æŠ‘åˆ¶SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class BaseScraper(ABC):
    """
    åŸºç¡€æŠ“å–å™¨ç±»ï¼ŒåŒ…å«æ‰€æœ‰å…±äº«åŠŸèƒ½
    å­ç±»åªéœ€è¦å®ç°ç‰¹å®šçš„è§£æé€»è¾‘
    """
    
    def __init__(self, config: Dict, scraper_name: str, sent_ids_file: str):
        """
        åˆå§‹åŒ–åŸºç¡€æŠ“å–å™¨
        
        Args:
            config: é…ç½®å­—å…¸
            scraper_name: æŠ“å–å™¨åç§°ï¼ˆç”¨äºæ—¥å¿—æ–‡ä»¶åï¼‰
            sent_ids_file: å·²å‘é€IDæ–‡ä»¶è·¯å¾„
        """
        self.config = config
        self.scraper_name = scraper_name
        self.sent_ids_file = sent_ids_file
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        self.project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.logs_dir = os.path.join(self.project_root, "logs")
        os.makedirs(self.logs_dir, exist_ok=True)
        
        self.sent_ids = self._load_sent_ids()
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
    
    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        logger.remove()  # ç§»é™¤é»˜è®¤å¤„ç†å™¨
        
        # ä¿¡æ¯æ—¥å¿—
        logger.add(
            os.path.join(self.logs_dir, f"{self.scraper_name}_info.log"),
            rotation="10 MB",
            retention="30 days",
            level="INFO",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
            encoding="utf-8",
            filter=lambda record: record["level"].name in ["INFO", "SUCCESS", "WARNING"]
        )
        
        # é”™è¯¯æ—¥å¿—
        logger.add(
            os.path.join(self.logs_dir, f"{self.scraper_name}_error.log"),
            rotation="5 MB",
            retention="60 days",
            level="ERROR",
            format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}",
            encoding="utf-8",
            filter=lambda record: record["level"].name in ["ERROR", "CRITICAL"]
        )
        
        # æ§åˆ¶å°è¾“å‡º
        logger.add(
            sys.stdout,
            level="INFO",
            format="{time:HH:mm:ss} | {level} | {message}",
            colorize=True
        )
    
    def _load_sent_ids(self) -> Set[str]:
        """ä»æ–‡ä»¶ä¸­åŠ è½½å·²å‘é€çš„IDé›†åˆ"""
        if os.path.exists(self.sent_ids_file):
            try:
                with open(self.sent_ids_file, 'r', encoding='utf-8') as f:
                    return set(json.load(f))
            except (json.JSONDecodeError, IOError) as e:
                logger.error(f"åŠ è½½å·²å‘é€IDæ–‡ä»¶å¤±è´¥ {self.sent_ids_file}: {e}. åˆå§‹åŒ–ä¸ºç©ºé›†åˆã€‚")
                return set()
        return set()
    
    def _save_sent_ids(self):
        """å°†å·²å‘é€çš„IDé›†åˆä¿å­˜åˆ°æ–‡ä»¶"""
        try:
            with open(self.sent_ids_file, 'w', encoding='utf-8') as f:
                json.dump(list(self.sent_ids), f, ensure_ascii=False, indent=4)
        except IOError as e:
            logger.error(f"ä¿å­˜å·²å‘é€IDæ–‡ä»¶å¤±è´¥ {self.sent_ids_file}: {e}")
    
    def fetch_html(self, url: str) -> Optional[str]:
        """è·å–ç½‘é¡µHTMLå†…å®¹"""
        try:
            logger.info(f"æ­£åœ¨è¯·æ±‚: {url}")
            response = requests.get(url, headers=self.config["HEADERS"], timeout=15, verify=False)
            response.raise_for_status()
            return response.text
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚ {url} å¤±è´¥: {e}")
            return None
    
    def fetch_job_description(self, job_url: str) -> str:
        """è·å–æ‹›è˜ä¿¡æ¯çš„è¯¦ç»†æè¿°"""
        html = self.fetch_html(job_url)
        if not html:
            return ""
        soup = BeautifulSoup(html, 'html.parser')
        postbody = soup.find('div', class_='postbody')
        if postbody:
            return postbody.get_text(separator='\n', strip=True)
        return ""
    
    def filter_new_jobs(self, all_jobs: List[Dict]) -> Tuple[List[Dict], Set[str]]:
        """è¿‡æ»¤å‡ºæ–°æ‹›è˜ä¿¡æ¯å¹¶æ›´æ–°å·²å‘é€IDé›†åˆ"""
        new_jobs = []
        updated_ids = set(self.sent_ids)
        
        for job in all_jobs:
            unique_id = f"{job['title']}-{job['link']}"
            if unique_id not in updated_ids:
                new_jobs.append(job)
                updated_ids.add(unique_id)
                logger.info(f"å‘ç°æ–°æ‹›è˜ä¿¡æ¯: {job['title']}")
        
        return new_jobs, updated_ids
    
    def parse_date(self, date_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ï¼Œè¿”å›å¯æ¯”è¾ƒçš„æ—¥æœŸå¯¹è±¡"""
        if not date_str:
            return None
        try:
            # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
            date_str = date_str.strip()
            
            # å°è¯•è§£æä¸åŒçš„æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2024-01-15
                r'(\d{1,2})/(\d{1,2})/(\d{4})',  # 01/15/2024 æˆ– 1/15/2024
                r'(\d{1,2})-(\d{1,2})-(\d{4})',  # 01-15-2024
                r'(\d{4})/(\d{1,2})/(\d{1,2})',  # 2024/01/15
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, date_str)
                if match:
                    if len(match.group(1)) == 4:  # å¹´ä»½åœ¨å‰
                        year, month, day = match.groups()
                    else:  # æœˆä»½åœ¨å‰
                        month, day, year = match.groups()
                    
                    # ç¡®ä¿æœˆæ—¥éƒ½æ˜¯ä¸¤ä½æ•°
                    month = month.zfill(2)
                    day = day.zfill(2)
                    
                    return datetime(int(year), int(month), int(day))
            
            # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›None
            return None
        except:
            return None
    
    def sort_jobs_by_date(self, jobs: List[Dict]) -> List[Dict]:
        """æ ¹æ®å‘å¸ƒæ—¥æœŸæ’åºï¼Œæœ€æ–°çš„æ’åœ¨å‰é¢"""
        try:
            def sort_key(job):
                """æ’åºé”®å‡½æ•°ï¼Œå¤„ç†Noneæ—¥æœŸ"""
                date_obj = self.parse_date(job.get('date', ''))
                # å¦‚æœæ—¥æœŸä¸ºNoneï¼Œè¿”å›ä¸€ä¸ªå¾ˆæ—©çš„æ—¥æœŸä½œä¸ºfallback
                if date_obj is None:
                    return (datetime(1900, 1, 1), 0)  # å¾ˆæ—©çš„æ—¥æœŸï¼Œæ’åœ¨æœ€å
                return (date_obj, 0)  # æ­£å¸¸æ—¥æœŸ
            
            jobs.sort(key=sort_key, reverse=True)
            
            # ç§»é™¤æ— æ³•è§£ææ—¥æœŸçš„é¡¹ç›®ï¼ˆæ”¾åœ¨æœ€åï¼‰
            valid_jobs = [job for job in jobs if self.parse_date(job.get('date', '')) is not None]
            invalid_jobs = [job for job in jobs if self.parse_date(job.get('date', '')) is None]
            return valid_jobs + invalid_jobs
        except Exception as e:
            logger.error(f"æ’åºæ‹›è˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return jobs
    
    def summarize_jobs_by_date(self, jobs: List[Dict]) -> str:
        """æŒ‰æ—¥æœŸç»Ÿè®¡å·¥ä½œæ•°é‡å¹¶ç”Ÿæˆæ€»ç»“"""
        # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
        date_counts = defaultdict(int)
        total_jobs = len(jobs)
        
        for job in jobs:
            date_str = job.get('date', '')
            if date_str:
                # å°è¯•è§£ææ—¥æœŸ
                try:
                    parsed_date = self.parse_date(date_str)
                    if parsed_date:
                        # æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD
                        formatted_date = parsed_date.strftime('%Y-%m-%d')
                        date_counts[formatted_date] += 1
                    else:
                        # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åŸå§‹æ—¥æœŸå­—ç¬¦ä¸²
                        date_counts[date_str] += 1
                        
                except Exception as e:
                    logger.warning(f"è§£ææ—¥æœŸå¤±è´¥ '{date_str}': {e}")
                    date_counts[date_str] += 1
            else:
                # æ²¡æœ‰æ—¥æœŸçš„å½’ä¸º"æœªçŸ¥æ—¥æœŸ"
                date_counts["æœªçŸ¥æ—¥æœŸ"] += 1
        
        # ç”Ÿæˆç»Ÿè®¡æ€»ç»“
        summary = f"\nğŸ“Š å·¥ä½œç»Ÿè®¡æ€»ç»“:\n"
        summary += f"æ€»å·¥ä½œæ•°é‡: {total_jobs} ä¸ª\n"
        summary += f"æŒ‰æ—¥æœŸåˆ†å¸ƒ:\n"
        
        if date_counts:
            # æŒ‰æ—¥æœŸæ’åºï¼ˆæœªçŸ¥æ—¥æœŸæ”¾åœ¨æœ€åï¼‰
            sorted_dates = sorted(
                date_counts.items(),
                key=lambda x: (x[0] == "æœªçŸ¥æ—¥æœŸ", x[0])
            )
            
            for date, count in sorted_dates:
                summary += f"  â€¢ {date}: {count} ä¸ªå·¥ä½œ\n"
        else:
            summary += f"  â€¢ æš‚æ— æ—¥æœŸä¿¡æ¯\n"
        
        summary += f"\n"
        return summary
    
    def format_email_body(self, jobs: List[Dict], search_terms: List[str]) -> Tuple[str, str]:
        """æ ¹æ®æ‹›è˜ä¿¡æ¯åˆ—è¡¨æ ¼å¼åŒ–é‚®ä»¶ä¸»é¢˜å’Œå†…å®¹"""
        search_terms_str = "ã€".join(search_terms[:3])
        subject = f"ã€{self.get_email_subject_prefix()}ã€‘{search_terms_str}ç­‰æ‹›è˜ä¿¡æ¯é€šçŸ¥"
        
        # ç”Ÿæˆç»Ÿè®¡æ€»ç»“
        summary = self.summarize_jobs_by_date(jobs)
        
        body = f"ä½ å¥½ï¼\n\næˆ‘ä»¬å‘ç°ä»¥ä¸‹æ–°çš„{self.get_job_type_name()}æ‹›è˜ä¿¡æ¯ï¼ˆåŒ¹é…å…³é”®è¯ï¼š{', '.join(search_terms)}ï¼‰ï¼š\n"
        body += summary  # æ·»åŠ ç»Ÿè®¡æ€»ç»“
        
        for idx, job in enumerate(jobs):
            body += f"{idx + 1}. ğŸ“… å‘å¸ƒæ—¥æœŸ: {job['date']}\n"
            body += f"   ğŸ“ æ ‡é¢˜: {job['title']}\n"
            body += f"   ğŸ‘¤ å‘å¸–äºº: {job['author']}\n"
            body += f"   ğŸ”— é“¾æ¥: {job['link']}\n"
            if 'desc' in job and job['desc']:
                body += f"   ğŸ“„ è¯¦æƒ…: {job['desc']}\n"
            body += "   " + "â”€" * 50 + "\n"
        
        body += "\nè¯·å°½å¿«æŸ¥çœ‹ï¼\n\n"
        body += f"é€šçŸ¥æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}"
        return subject, body
    
    def send_email(self, subject: str, body: str):
        """å‘é€é‚®ä»¶é€šçŸ¥"""
        msg = MIMEText(body, 'plain', 'utf-8')
        msg['Subject'] = subject
        msg['From'] = self.config["EMAIL"]["SENDER_EMAIL"]
        msg['To'] = self.config["EMAIL"]["RECEIVER_EMAIL"]

        try:
            if self.config["EMAIL"]["SMTP_PORT"] == 465:
                server = smtplib.SMTP_SSL(self.config["EMAIL"]["SMTP_SERVER"], self.config["EMAIL"]["SMTP_PORT"])
            else:
                server = smtplib.SMTP(self.config["EMAIL"]["SMTP_SERVER"], self.config["EMAIL"]["SMTP_PORT"])
                server.starttls()
            
            server.login(self.config["EMAIL"]["SENDER_EMAIL"], self.config["EMAIL"]["SENDER_PASSWORD"])
            server.send_message(msg)
            server.quit()
            logger.success(f"é‚®ä»¶å·²å‘é€åˆ° {self.config['EMAIL']['RECEIVER_EMAIL']}")
        except Exception as e:
            logger.error(f"å‘é€é‚®ä»¶å¤±è´¥: {e}")
            logger.error(f"è¯·æ£€æŸ¥é‚®ç®±é…ç½®ï¼šå‘ä»¶äººé‚®ç®±ã€å¯†ç ã€SMTPæœåŠ¡å™¨å’Œç«¯å£ã€‚")
    
    def scrape_and_notify(self) -> Set[str]:
        """ä¸»åè°ƒå‡½æ•°ï¼šæ‰§è¡Œæ‹›è˜ä¿¡æ¯æŠ“å–ã€ç­›é€‰å’Œé€šçŸ¥çš„æ•´ä¸ªæµç¨‹"""
        try:
            logger.info(f"--- {self.get_job_type_name()}æ‹›è˜ç›‘æ§ä»»åŠ¡å¼€å§‹: {time.strftime('%Y-%m-%d %H:%M:%S')} ---")
            all_raw_jobs = []

            # è·å–é¡µé¢URLåˆ—è¡¨
            urls = self.get_target_urls()

            for page_num, page_url in enumerate(urls, 1):
                try:
                    logger.info(f"æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ: {page_url}")
                    html_content = self.fetch_html(page_url)
                    if html_content:
                        # è·å–æœç´¢å…³é”®è¯
                        keywords = self.get_search_keywords()
                        jobs_on_page = self.parse_html_for_jobs(html_content, page_url, keywords)
                        all_raw_jobs.extend(jobs_on_page)
                    else:
                        logger.warning(f"ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥ï¼Œè·³è¿‡")
                except Exception as e:
                    logger.error(f"æŠ“å–ç¬¬ {page_num} é¡µæ—¶å‡ºé”™: {e}")
                    continue
                
                time.sleep(2)  # ç¤¼è²Œæ€§å»¶è¿Ÿ

            # è¿‡æ»¤å‡ºæ–°æ‹›è˜ä¿¡æ¯å¹¶æ›´æ–°å·²å‘é€IDé›†åˆ
            try:
                new_jobs, updated_sent_ids = self.filter_new_jobs(all_raw_jobs)
            except Exception as e:
                logger.error(f"è¿‡æ»¤æ‹›è˜ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                return self.sent_ids

            # è·å–æ¯ä¸ªæ–°æ‹›è˜ä¿¡æ¯çš„è¯¦ç»†æè¿°
            for job in new_jobs:
                try:
                    job['desc'] = self.fetch_job_description(job['link'])
                except Exception as e:
                    logger.warning(f"è·å–æ‹›è˜è¯¦æƒ…å¤±è´¥: {e}")
                    job['desc'] = "è¯¦æƒ…è·å–å¤±è´¥"

            # æ ¹æ®å‘å¸ƒæ—¥æœŸæ’åºï¼Œæœ€æ–°çš„æ’åœ¨å‰é¢
            new_jobs = self.sort_jobs_by_date(new_jobs)

            if new_jobs:
                # æ ¼å¼åŒ–é‚®ä»¶å†…å®¹
                try:
                    keywords = self.get_search_keywords()
                    
                    # ç”Ÿæˆå¹¶è®°å½•ç»Ÿè®¡ä¿¡æ¯
                    summary = self.summarize_jobs_by_date(new_jobs)
                    logger.info(f"ğŸ“Š å‘ç°æ–°å·¥ä½œç»Ÿè®¡: {summary.strip()}")
                    
                    subject, body = self.format_email_body(new_jobs, keywords)
                    
                    # æ‰“å°é‚®ä»¶å†…å®¹é¢„è§ˆ
                    logger.info(f"\nğŸ“§ é‚®ä»¶é¢„è§ˆ:")
                    logger.info(f"ä¸»é¢˜: {subject}")
                    logger.info(f"æ”¶ä»¶äºº: {self.config['EMAIL']['RECEIVER_EMAIL']}")
                    logger.info(f"å†…å®¹:\n{body}")
                    logger.info(f"--- é‚®ä»¶é¢„è§ˆç»“æŸ ---\n")
                    
                    # å‘é€é‚®ä»¶
                    self.send_email(subject, body)
                except Exception as e:
                    logger.error(f"å‘é€é‚®ä»¶æ—¶å‡ºé”™: {e}")
            else:
                try:
                    keywords = self.get_search_keywords()
                    search_terms_str = "ã€".join(keywords[:3])
                    logger.info(f"æ­¤æ¬¡ä»»åŠ¡æœªå‘ç°æ–°çš„ '{search_terms_str}ç­‰' æ‹›è˜ä¿¡æ¯ã€‚")
                except Exception as e:
                    logger.error(f"å¤„ç†æ— æ–°ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            
            logger.info(f"--- {self.get_job_type_name()}æ‹›è˜ç›‘æ§ä»»åŠ¡ç»“æŸ: {time.strftime('%Y-%m-%d %H:%M:%S')} ---\n")
            return updated_sent_ids
            
        except Exception as e:
            logger.error(f"{self.get_job_type_name()}æ‹›è˜ç›‘æ§ä»»åŠ¡å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            logger.error(f"ç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œç­‰å¾…ä¸‹æ¬¡è°ƒåº¦...")
            return self.sent_ids
    
    def run_scheduled_task(self):
        """è°ƒåº¦ä»»åŠ¡å‡½æ•°"""
        try:
            self.sent_ids = self.scrape_and_notify()
            self._save_sent_ids()
        except Exception as e:
            logger.error(f"è°ƒåº¦ä»»åŠ¡å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            logger.error(f"ç¨‹åºå°†ç»§ç»­è¿è¡Œï¼Œç­‰å¾…ä¸‹æ¬¡è°ƒåº¦...")
            # å³ä½¿å‡ºé”™ä¹Ÿè¦ä¿å­˜å½“å‰çŠ¶æ€
            try:
                self._save_sent_ids()
            except Exception as save_error:
                logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {save_error}")
    
    # æŠ½è±¡æ–¹æ³• - å­ç±»å¿…é¡»å®ç°
    @abstractmethod
    def get_target_urls(self) -> List[str]:
        """è·å–ç›®æ ‡URLåˆ—è¡¨"""
        pass
    
    @abstractmethod
    def get_search_keywords(self) -> List[str]:
        """è·å–æœç´¢å…³é”®è¯åˆ—è¡¨"""
        pass
    
    @abstractmethod
    def parse_html_for_jobs(self, html_content: str, base_url: str, search_terms: List[str]) -> List[Dict]:
        """è§£æHTMLå†…å®¹ï¼Œæå–ç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯"""
        pass
    
    @abstractmethod
    def get_job_type_name(self) -> str:
        """è·å–å·¥ä½œç±»å‹åç§°ï¼ˆç”¨äºæ—¥å¿—å’Œé‚®ä»¶ï¼‰"""
        pass
    
    @abstractmethod
    def get_email_subject_prefix(self) -> str:
        """è·å–é‚®ä»¶ä¸»é¢˜å‰ç¼€"""
        pass 